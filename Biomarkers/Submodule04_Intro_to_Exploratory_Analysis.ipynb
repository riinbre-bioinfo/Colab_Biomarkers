{"cells":[{"cell_type":"markdown","id":"dd89e273-6760-4654-8014-71ac66b08a38","metadata":{"id":"dd89e273-6760-4654-8014-71ac66b08a38"},"source":["<table>\n","  <tr>\n","    <th>\n","      <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/RIINBRE-Logo.jpg\", height = \"125\", alt=\"RI-INBRE Logo\">\n","    </th>\n","    <th>\n","      <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/MIC_Logo.png\", height = \"125\", alt=\"RI-INBRE Logo\">\n","    </th>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","id":"e5a71e1d-cc9e-4527-bff0-27cf4eb5ed48","metadata":{"id":"e5a71e1d-cc9e-4527-bff0-27cf4eb5ed48"},"source":["---"]},{"cell_type":"markdown","id":"43c08b43-5eb3-4fc2-a2f9-0ace910cfef3","metadata":{"id":"43c08b43-5eb3-4fc2-a2f9-0ace910cfef3"},"source":["# Analysis of Biomedical Data for Biomarker Discovery\n","## Submodule 4: Introduction to Exploratory Analysis\n","### Dr. Christopher L. Hemme\n","### Director, [RI-INBRE Molecular Informatics Core](https://web.uri.edu/riinbre/mic/)\n","### The University of Rhode Island College of Pharmacy\n","Last Updated: June 5, 2023"]},{"cell_type":"markdown","id":"2c80b7b6-f550-406f-ab15-3b52ccf950f1","metadata":{"id":"2c80b7b6-f550-406f-ab15-3b52ccf950f1"},"source":["---"]},{"cell_type":"markdown","id":"36d8f98c-8593-4359-9bc7-4b9b2b80d95a","metadata":{"id":"36d8f98c-8593-4359-9bc7-4b9b2b80d95a"},"source":["## Introduction"]},{"cell_type":"markdown","id":"d0832c47-fbe1-4e3f-87b6-0d26f4a02688","metadata":{"id":"d0832c47-fbe1-4e3f-87b6-0d26f4a02688"},"source":["Once data has been generated from experiments, we begin the process of data analysis.  Traditionally this consisted primarily of <b>statistical hypothesis testing</b>, in which the scientist generates a set of null and alternative hypotheses which can be tested experimentally.  For example, let's say we're testing a new Wonder Drug B to see if it reduces blood pressure better than Wonder Drug A.  We might set up the following null ($H_0$) and alternative ($H_1$) hypotheses:\n","\n","$$H_0 = \\text{Wonder Drug B does not significantly lower blood pressure compared to Wonder Drug A}$$\n","$$H_1 = \\text{Wonder Drug B significantly lowers blood pressure compared to Wonder Drug A}$$\n","\n","We would then set up the experiment, analyze it with a one-tailed <i>t</i>-test at a significance level of <i>p</i> = 0.05, and then determine if the null hypothesis can be rejected.  Statistical hypothesis testing is a well-established method, and while it has its short-comings, is still a cornerstone of data analysis.\n","\n","In recent decades, increased emphasis has been placed on <b>exploratory data analysis</b> or <b>EDA</b>.  Intended as a compliment to statistical hypothesis testing, EDA gives us a broader, bird's eye view of the data that can be used to guide hypothesis building, identify hidden trends in data, and to suggest statistical methods for analysis.  EDA can range from simple methods such as scatter plots and heatmaps to more complex methods such as dimensionality reduction and clustering.  Visualization is a key component of EDA, particularly for large high-dimensional datasets that would be difficult to visualize using traditional methods.\n","\n","EDA is very common when analyzing omics data, which is usually high-dimensional data with many of the variables highly correlated.  In this submodule, we will look at some common general methods of EDA that you will likely encounter when working with omics data, including heatmaps and principal component analysis.  Methods more specific to omics data such as volcano plots and MA plots are discussed in <b>Submodule 4: Exploratory Analysis of Proteomics IRI Data</b>."]},{"cell_type":"markdown","id":"4b9e6dc3-2ea0-48be-930f-27cc49223379","metadata":{"id":"4b9e6dc3-2ea0-48be-930f-27cc49223379"},"source":["<div class=\"alert alert-block alert-info\">\n","<b>&#9995; Tip:</b> Blue boxes will indicate helpful tips.</div>"]},{"cell_type":"markdown","id":"22eada68-df92-4f45-802a-92a3e6615633","metadata":{"id":"22eada68-df92-4f45-802a-92a3e6615633"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> Used for interesting asides or notes.\n","</div>"]},{"cell_type":"markdown","id":"50348e42-0b42-44c7-aa4f-c47230682cea","metadata":{"id":"50348e42-0b42-44c7-aa4f-c47230682cea"},"source":["<div class=\"alert alert-block alert-success\">\n","<b>&#9997; Reference:</b> This box indicates a reference for an attached figure or table.\n","</div>"]},{"cell_type":"markdown","id":"1504b86e-cccd-4a38-b1c6-c56f0af2e46a","metadata":{"id":"1504b86e-cccd-4a38-b1c6-c56f0af2e46a"},"source":["<div class=\"alert alert-block alert-danger\">\n","<b>&#128721; Caution:</b> A red box indicates potential hazards or pitfalls you may encounter.\n","</div>"]},{"cell_type":"markdown","id":"e8eaff7b-f00e-43a1-83ae-229e9a931f61","metadata":{"id":"e8eaff7b-f00e-43a1-83ae-229e9a931f61"},"source":["---"]},{"cell_type":"markdown","id":"f1d8666c-d158-4996-a8c9-89d585b0a165","metadata":{"id":"f1d8666c-d158-4996-a8c9-89d585b0a165"},"source":["## Load R Modules"]},{"cell_type":"code","execution_count":null,"id":"5d15e119-9be0-477e-a6a2-929fa846eb27","metadata":{"id":"5d15e119-9be0-477e-a6a2-929fa846eb27"},"outputs":[],"source":["packages <- c(\"heatmaply\", \"GGally\", \"plotly\")\n","installed_packages <- packages %in% rownames(installed.packages())\n","if (any(installed_packages == FALSE)) {install.packages(packages[!installed_packages])}\n","\n","# plotly creates interactive web graphics\n","# heatmaply creates interactive heatmaps based on plotly and ggplot2\n","# GGally is an extension of ggplot2"]},{"cell_type":"code","execution_count":null,"id":"95ea4ff0-b503-4f67-97aa-2fc9382569e1","metadata":{"id":"95ea4ff0-b503-4f67-97aa-2fc9382569e1"},"outputs":[],"source":["require('tidyverse')\n","require('plotly')\n","require('heatmaply')\n","require('RColorBrewer')\n","require('GGally')\n","require('factoextra')\n","require('cluster')\n","require('dendextend')"]},{"cell_type":"code","execution_count":null,"id":"28e32789-7c61-4884-b678-0d9deda61de4","metadata":{"id":"28e32789-7c61-4884-b678-0d9deda61de4"},"outputs":[],"source":["# factoextra visualizes results of multivariate data analysis\n","# This must be done after the require() statements in order to avoid overwriting certain packages\n","if(!require(devtools)) install.packages(\"devtools\")\n","devtools::install_github(\"kassambara/factoextra\")\n","library(\"factoextra\")"]},{"cell_type":"markdown","id":"65f4d5af-6b97-4bca-a5bc-b0ff2828417b","metadata":{"id":"65f4d5af-6b97-4bca-a5bc-b0ff2828417b"},"source":["---"]},{"cell_type":"markdown","id":"a558273d-528d-4946-b589-20a2a302b14b","metadata":{"id":"a558273d-528d-4946-b589-20a2a302b14b"},"source":["## Simple Descriptive Graphs"]},{"cell_type":"markdown","id":"09941a4f-3721-4d9a-8768-560d4da5d160","metadata":{"id":"09941a4f-3721-4d9a-8768-560d4da5d160"},"source":["For low dimensionality data, simple graphs are often sufficient to highlight properties of the data.  Consider the <b>iris</b> dataset built into R which describes three species of iris based on four physical properties."]},{"cell_type":"code","execution_count":null,"id":"0f930924-a32b-40b7-8b83-3fd71a5cb4c7","metadata":{"id":"0f930924-a32b-40b7-8b83-3fd71a5cb4c7"},"outputs":[],"source":["iris # print out iris data\n","summary(iris) # calculate summary statistics for iris data"]},{"cell_type":"markdown","id":"edf5cac0-7462-45ab-8b99-b787ac057e7b","metadata":{"id":"edf5cac0-7462-45ab-8b99-b787ac057e7b"},"source":["There are several basic plots we can use to summarize the data.  First, is the scatter plot of two variables.  If you completed <b>Submodule 7: Introduction to Linear Models</b>, we made a scatterplot of the Petal Length vs. Sepal Length and colored the data points by Species so that we could identify trends in the data before conducting linear regression."]},{"cell_type":"code","execution_count":null,"id":"c7f5e569-519d-4fee-8caf-0af9ffdb9bd5","metadata":{"id":"c7f5e569-519d-4fee-8caf-0af9ffdb9bd5"},"outputs":[],"source":["iris_scatter <- iris %>%\n","    ggplot(aes(x = Sepal.Length, y = Petal.Length)) +\n","        geom_point(size = 3, aes(color = Species, shape = Species)) +\n","        xlab(\"Sepal Length\") +\n","        ylab(\"Petal Length\") +\n","        geom_smooth(method=\"lm\", aes(fill=Species)) +\n","        theme_bw() +\n","        theme(text=element_text(size = 24))\n","iris_scatter\n","\n","# basic scatter plot (geom_point) that sets the colors and shape of the data points by Species\n","# We add a trend line (geom_smooth) based on linear regression (method=\"lm\") using the default formula y ~ x\n","# We modify geom_smooth with an aesthetic (aes(fill=Species)) to fit each Species separately"]},{"cell_type":"markdown","id":"d3fecfdd-eee3-4f7c-a1dc-3cbdc7473ad6","metadata":{"id":"d3fecfdd-eee3-4f7c-a1dc-3cbdc7473ad6"},"source":["We can clearly see that each species has a different relationship between the two variables.  Another way of dealing with this data is with a box plot."]},{"cell_type":"code","execution_count":null,"id":"66a13d99-4356-4b73-86b6-f04cfacaaa86","metadata":{"id":"66a13d99-4356-4b73-86b6-f04cfacaaa86"},"outputs":[],"source":["iris_box <- iris %>%\n","    ggplot(aes(x = Species, y = Petal.Length)) +\n","        geom_boxplot(aes(color = Species)) +\n","        xlab(\"Sepal Length\") +\n","        ylab(\"Petal Length\") +\n","        theme_bw() +\n","        theme(text=element_text(size = 24))\n","iris_box\n","\n","# basic box plot (geom_boxplot) that sets the colors boxes by Species"]},{"cell_type":"markdown","id":"a79b02f6-b9e0-47c0-8150-322c85b7604c","metadata":{"id":"a79b02f6-b9e0-47c0-8150-322c85b7604c"},"source":["The box plot (also called a box and whisker plot) is extremely useful for visualizing descriptive statistics and identifying outliers.  How do we interpret this plot?  The box scales between the 1st and 3rd quartiles, representing ~50% of the data (also called the <b>interquartile range</b> or <b>IQR</b>.  This gives an indication of the spread of the data.  The whiskers indicate the maximum and minimum values of the data and identifies outliers which are presented as dots.  Outliers are points that lie outside of an established range, usually outside $\\pm1.5\\;\\text{IGR}$.  The line in the middle of the box is the median, that is, the middle value that separates 50% of the data.\n","\n","Let's add the actual data points.  We'll add a bit of noise to the data using <b>geom_jitter</b> to make it easier to see the individual points."]},{"cell_type":"code","execution_count":null,"id":"540752ac-810c-4bc1-b670-ce6a106d75ec","metadata":{"id":"540752ac-810c-4bc1-b670-ce6a106d75ec"},"outputs":[],"source":["iris_box + geom_jitter(shape=16, position=position_jitter(0.2))\n","\n","# The shape argument sets the shape to filled circle.  We can adjust the position_jitter number to add more or less noise."]},{"cell_type":"markdown","id":"ff01f0fd-cff1-4e19-9008-ea50768dd0ed","metadata":{"id":"ff01f0fd-cff1-4e19-9008-ea50768dd0ed"},"source":["We can see the separation of the data by Species very clearly with the box plot.  We can also predict what the results of further analysis such as ANOVA will tell us, i.e., that significant differences between the means of the three species exist.  Often you might see box plots attached to other plots (e.g., placed on the axis of a heatmap) or overlaid with additional information (e.g., ANOVA results).  Box plots are also useful in visualizing data normalization to determine if your data is ready for further analysis.  That said, we need to be careful in interpreting the box plot.  Data with a large spread may look significantly different when plotting but may show no statistically significant differences when analyzed by more comprehensive statistical methods.  Like all exploratory analysis, the box plot is a guide that helps to understand patterns in your data but needs to be backed up with more rigorous methods."]},{"cell_type":"markdown","id":"fd05d26d-b4ca-4f90-9ee5-2cf612fb6c97","metadata":{"id":"fd05d26d-b4ca-4f90-9ee5-2cf612fb6c97"},"source":["---"]},{"cell_type":"markdown","id":"1ee6f650-a121-4b91-ae97-0db95cac1484","metadata":{"id":"1ee6f650-a121-4b91-ae97-0db95cac1484"},"source":["### Heatmaps"]},{"cell_type":"markdown","id":"48f3b077-6732-409f-8580-fac564783e67","metadata":{"id":"48f3b077-6732-409f-8580-fac564783e67"},"source":["Another useful visualization tool, especially for big data sets, is the heatmap.  Consider a bar plot in three dimensions, with the vertical dimension representing some response variable.  A 3D bar plot is difficult to visualize on a 2D screen which makes interpretation difficult.  But what if we color the bars based on the value of the response variable and then look at the plot from above?  Now we have a two dimensional grid where the value of the response variable has been converted to color, with the color range representing the range of values.  Not only is this plot easier to read, but by clustering the rows and columns of the plot, we can identify global patterns in the data that are not immediately obvious when visualizing it in other ways.  This is the basis of the heatmap, a very common visualization format in omics data analysis. "]},{"cell_type":"markdown","id":"bc63b591-ae30-441c-997f-243d5ca9c383","metadata":{"id":"bc63b591-ae30-441c-997f-243d5ca9c383"},"source":["<div class=\"alert alert-block alert-info\">\n","<b>&#9995; Tip:</b> Because the heatmap is such a common visualization tools, there are dozens of packages available in R for creating them.  We'll use the <b>heatmaply</b> package here and in <b>Submodule 5: Identification of IRI Biomarkers from Proteomic Data</b>, we'll use the <b>ComplexHeatmap</b> package which is specialized for omics data.  <b>heatmaply</b> is compatable with <b>ggplot2</b> and tidy principles and allows us to create interactive heatmaps.  <b>heatmaply</b> allows you to zoom in on regions of the global heatmap to get a better view of particular regions.</div>"]},{"cell_type":"markdown","id":"720156a4-1d5b-4d32-af93-243332719090","metadata":{"id":"720156a4-1d5b-4d32-af93-243332719090"},"source":["<div class=\"alert alert-block alert-danger\">\n","<b>&#128721; Caution:</b> While heatmaps are useful for visualizing big data sets, they can also become very crowded and difficult to read.  Also, the choice of colors is critical to interpreting the heatmap.  A poor choice of colors can lead to confusion, imply false patterns, or complicate visualization by color-blind viewers.\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"624fa3d9-00cb-4b22-ac77-33056cbaed98","metadata":{"id":"624fa3d9-00cb-4b22-ac77-33056cbaed98"},"outputs":[],"source":["#iris_subset <- iris[1:4] # limit to numerical columns\n","#heatmaply(iris_subset) # plot heatmap\n","\n","# heatmaply may take a few seconds to create the heatmap, particularly for large datasets.\n","\n","library(plotly)\n","library(ggplot2)\n","df = data.frame(X =c(1:5),  Y=c(2:6) )\n","gg <- ggplot(df, aes(x=X,y=Y))+geom_point()\n","ggplotly(gg)\n","ggplotly(gg)"]},{"cell_type":"markdown","id":"07bbd87b-60fe-4951-86e3-4aeebb6dc860","metadata":{"id":"07bbd87b-60fe-4951-86e3-4aeebb6dc860"},"source":["We can start to see patterns in the data, but the range is very broad.  In some omics datasets where the range of values is very large and the count matrix is sparse (e.g., many cells are zero), a heatmap of the raw data will be a giant uninformative blob of the low range color.  To better distinguish the differences, we should first normalize the data.  There are many ways to normalize data, but for this dataset, we will use <b>heatmaply's</b> built-in <b>normalize</b> function which normalizes the data to between 0 and 1 by subtracting the minimum value of the dataset from each data point and dividing by the maximum value."]},{"cell_type":"code","execution_count":null,"id":"85e62400-862f-4a36-9d92-34083e6548ba","metadata":{"id":"85e62400-862f-4a36-9d92-34083e6548ba"},"outputs":[],"source":["iris_norm <- normalize(iris_subset) # normalization function\n","heatmaply(iris_norm)"]},{"cell_type":"markdown","id":"f49c34ca-5915-49fe-ba4c-f4bd31c87c5e","metadata":{"id":"f49c34ca-5915-49fe-ba4c-f4bd31c87c5e"},"source":["Now the patterns are more clear.  We can see that the Sepal.Width data is distinct from the other three variables, and that Petal.Length and Petal.Width are similar to each other.  The tree-like structures on the top and right are <b>dendrograms</b> that indicate relationships between the columns and rows, respectively.  The rows and columns have been rearranged through <b>hierarchical clustering</b> to place similar rows or columns together.  This can be very useful in pattern identification because it tends to create a box-like structure to the heatmap.  Sometimes though, we might not want this clustering to occur.  For example, we might want similar samples to be together so we can see patterns within the group, or we might want genes arranged in chromosomal order.  In such cases, we can turn the clustering off.\n","\n","In our case, we'll leave the clustering on because we want to see if the arrangement of the rows corresponds to species.  We can tell by the row dendrogram that there are 3-5 clusters, and we know from the raw data that we have 3 species.  Let's clean up our heatmap a bit to find out how well our data is really organized.  We'll recolor the heatmap using <b>RColorBrewer</b>, a popular package that provides a variety of color palettes.  We'll color the dendrogram based on clusters defined by <b>k-means clustering</b> and we'll set the number of clusters to 3 since we know that's how many species we have.  Finally, we will add a coloun that is colored by Species so that we can compare to the clusters chosen algorithmically."]},{"cell_type":"markdown","id":"cc2dbb8c-b56d-4f53-97e4-7e1007ba1f24","metadata":{"id":"cc2dbb8c-b56d-4f53-97e4-7e1007ba1f24"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> We'll discuss clustering methods later in this module.\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"d1267ea7-c0dc-4d97-bb5f-3366e62cc305","metadata":{"tags":[],"id":"d1267ea7-c0dc-4d97-bb5f-3366e62cc305"},"outputs":[],"source":["heatmaply(\n","    iris_norm,\n","    colors = colorRampPalette(brewer.pal(3, \"BuPu\")), # Uses \"Blue-Purple\" color palette\n","    k_row = 3, # k-means clustering assuming three clusters\n","    row_side_colors = iris[, 5] # sets row side colors based on Species\n",")"]},{"cell_type":"markdown","id":"e5d70b60-ad87-49cb-bcb4-84c99cad78ed","metadata":{"id":"e5d70b60-ad87-49cb-bcb4-84c99cad78ed"},"source":["As expected, one of the clusters defined by the k-means algorithm corresponds to the setosa samples, which seem to have larger than average sepal width but smaller sepal lengths, petal lengths, and petal widths.  The virginica and versicolor samples don't separate as cleanly.  Depending on the dataset, a pattern like this could indicate relevant subpopulations or it could just be the results of noise in the data.  In the case of iris, we can see from the scatterplots that while versicolor and virginica are distinct, there is still some overlap in the two groups."]},{"cell_type":"markdown","id":"1884196a-7092-400d-a02c-668453414c25","metadata":{"id":"1884196a-7092-400d-a02c-668453414c25"},"source":["---"]},{"cell_type":"markdown","id":"0eb1355e-b4db-49a5-a8b0-6a416d504dd8","metadata":{"id":"0eb1355e-b4db-49a5-a8b0-6a416d504dd8"},"source":["## Dimensionality Reduction"]},{"cell_type":"markdown","id":"91dcc788-c71a-4bef-87ff-8b6ddfc05749","metadata":{"id":"91dcc788-c71a-4bef-87ff-8b6ddfc05749"},"source":["The heatmaps begin to give us an idea of how the iris dataset is organized.  We can begin to see patterns in the data that indicate why the species differ.  In particular, Sepal Length, Petal Length and Petal Width seem to show similar patterns, suggesting they are correlated.  We can check this using a scatter plot matrix, that is, create scatterplots of every pair of variables.  We'll use the <b>plotly</b> and <b>ggally</b> packages to do this, specifically the <b>ggpairs</b> function."]},{"cell_type":"code","execution_count":null,"id":"fabcc0a7-0778-4e55-97db-0bd47584e450","metadata":{"id":"fabcc0a7-0778-4e55-97db-0bd47584e450"},"outputs":[],"source":["ggpairs(iris,\n","        aes(colour = Species)\n","       )"]},{"cell_type":"markdown","id":"e6dc55dd-5769-4ea9-8a19-dbe1de21b000","metadata":{"id":"e6dc55dd-5769-4ea9-8a19-dbe1de21b000"},"source":["There is a lot of data in this figure.  The diagonal shows density plots for each variable separated by species.  The right hand column shows the bar plots for each variable and the bottom row shows histograms for each variable.  The lower triangle shows the scatterplots and the upper triangle shows the correlation coefficients for each variable by species (correlation coefficient ranges between 0 (no correlation) and 1 (perfect correlation)).  We can now start to see patterns in the variables, particularly in which variables might be correlated (see the correlation coefficient for each pair).\n","\n","This is fine for four variables, but what if we were analyzing an omics data set with 4000 variables?  For high dimension data, the number of variables to consider is so high that it significantly decreases the accuracy of our models.  This is particularly problematic with omics data where we assume that most features won't be relevant to our models (e.g., most genes won't be differentially expressed between most conditions).  We call this problem the <b>curse of dimensionality</b>.  We could combat it by adding more data, but this isn't always feasible.  Alternatively, we can reduce the number of features, resulting in a simpler dataset where groups of correlated variables are transformed to a smaller number variables that best explain the system.  We call this process <b>dimensionality reduction</b> and the best known method is <b>principal component analysis (PCA)</b>."]},{"cell_type":"markdown","id":"902a8bba-8a0a-42cd-8fb9-831d84a8e27d","metadata":{"tags":[],"id":"902a8bba-8a0a-42cd-8fb9-831d84a8e27d"},"source":["Consider the following 2D scatter plot.  We can see that there is a correlation between these two variables and that both the x and y axes contribute to the variability in the sample. "]},{"cell_type":"markdown","id":"9009c232-d6ae-4a92-b71f-87860a655f87","metadata":{"id":"9009c232-d6ae-4a92-b71f-87860a655f87"},"source":["<div>\n","  <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/PCA1.jpg\", alt=\"PCA1\">\n","</div>"]},{"cell_type":"markdown","id":"1f2e5f21-8a41-4e1a-bca6-8ecd9b1c3bf9","metadata":{"id":"1f2e5f21-8a41-4e1a-bca6-8ecd9b1c3bf9"},"source":["The variability of the dataset can be described by the <b>covariance matrix</b> from which we can identify a new axis called a <b>principal component</b>, or <b>PC</b>.  The first PC (PC1) is oriented along the direction of the highest variance in the data.  When this variance is accounted for, a second PC (PC2) can be calculated along the direction of the second highest variance, and so on until all of the variance has been accounted for.  The data points are transformed based on the new variables represented by the PC's and the PC's are rotated to orient them along traditional horizontal and vertical lines that we're familiar with."]},{"cell_type":"markdown","id":"34b8e0cd-01a6-45d6-97a3-b41fb06dc5d2","metadata":{"id":"34b8e0cd-01a6-45d6-97a3-b41fb06dc5d2"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> To put it another way, the PC's are linear combinations of the observed variables.  The PCA algorithm is calculating a new origin and building a multidimensional oval around the data defined by the magnitude and length of the PC's.  It's important to note that unlike the original data points, the values of the transformed data points plotted on the PCA have no inherent experimental meaning.\n","</div>"]},{"cell_type":"markdown","id":"5c258234-86f8-4adf-b206-8af6b89fbe1c","metadata":{"id":"5c258234-86f8-4adf-b206-8af6b89fbe1c"},"source":["<div>\n","  <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/PCA2.jpg\", alt=\"PCA2\">\n","</div>\n"]},{"cell_type":"markdown","id":"60a1464d-7007-405e-9092-76c345c45525","metadata":{"id":"60a1464d-7007-405e-9092-76c345c45525"},"source":["<div>\n","  <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/PCA3.jpg\", alt=\"PCA3\">\n","</div>"]},{"cell_type":"markdown","id":"f6b231dc-5b61-4a98-984d-99565bbf380c","metadata":{"id":"f6b231dc-5b61-4a98-984d-99565bbf380c"},"source":["The data is transformed and rotated into a new coordinate system, where the new axes (the PC's) are independent of each other and define the amount of variable along a given axis.  In this hypothetical example, PC1 accounts for 80% of the variability and PC2 the remaining 20%.\n","\n","More formally, we are calculating the <b>eigenvectors</b> and <b>eigenvalues</b> of the covariance matrix.  The eigenvectors represent the orientation (direction) of the new PC's compared to the original data and the eigenvalues are the covariance (magnitude) along the new PC's.\n","\n","PCA is thus decomposing the data matrix <b>X</b> into matrices <b>U</b> and <b>V</b>, where\n","\n","$$X = A \\cdot B = AB^T$$\n","\n","$A$ = Scores matrix, which is the original data rotated into the new coordinate system\n","\n","$B$ = Loadings matrix, which is the weights applied to each data point in the new coordinate system"]},{"cell_type":"markdown","id":"9e878476-2a7b-446b-b183-5d9d4491ae7f","metadata":{"id":"9e878476-2a7b-446b-b183-5d9d4491ae7f"},"source":["Now let's use R's <b>prcomp</b> function to analyze the iris data set."]},{"cell_type":"markdown","id":"9e33265f-bd1a-4803-be17-53f7bf945f8f","metadata":{"id":"9e33265f-bd1a-4803-be17-53f7bf945f8f"},"source":["<div class=\"alert alert-block alert-info\">\n","<b>&#9995; Tip:</b> As with heatmaps, PCA is so commonly used that you can find dozens of packages for conducting and visualizing PCA.  We'll use base R functions here.</div>"]},{"cell_type":"code","execution_count":null,"id":"7fb387d7-6a61-4dae-af8a-fae99d743185","metadata":{"id":"7fb387d7-6a61-4dae-af8a-fae99d743185"},"outputs":[],"source":["iris.pca <- prcomp(iris[,1:4], scale = TRUE, center = TRUE) # run PCA analysis, scaling the data to unit variance and centering on zero.\n","summary(iris.pca) # summarize the PCA output\n","fviz_eig(iris.pca) # plot scree plot"]},{"cell_type":"markdown","id":"50db62c9-b16d-4b89-b82e-45fa585fdee8","metadata":{"id":"50db62c9-b16d-4b89-b82e-45fa585fdee8"},"source":["As with heatmaps, it's important to normalize data before running PCA.  Otherwise, the first PC will likely contain more variance than expected.  For this example, we'll simply scale the data using <b>prcomp</b>'s scale argument.  We can see from the output the individual and cumulative variance associated with each PC, and we can plot that as a <b>scree plot</b> (also called an <b>elbow plot</b>.  We can see that PC1 accounts for ~73% of the variance and PC2 23%, for a total of 95%.  This means that we can ignore PC3 and PC4 without losing much information.  Let's see what a scatterplot of PC1 and PC2 looks like."]},{"cell_type":"code","execution_count":null,"id":"59207961-3a03-4c02-b258-235d504bdb09","metadata":{"id":"59207961-3a03-4c02-b258-235d504bdb09"},"outputs":[],"source":["fviz_pca_ind(iris.pca,\n","             col.ind = iris$Species,\n","             repel = TRUE\n","             )\n","\n","# plot individual data points, colored by Species, using factoextra package"]},{"cell_type":"markdown","id":"fceaa4d4-3f5e-4833-a2f8-36e09e6eac46","metadata":{"id":"fceaa4d4-3f5e-4833-a2f8-36e09e6eac46"},"source":["All three species, especially setosa, are separated along PC1.  There is some separation along PC2, but it doesn't look particularly meaningful.  What's driving this separation?  To determine that, we need to look at the <b>loading plot</b>."]},{"cell_type":"code","execution_count":null,"id":"deb17d2d-2f0d-490e-864f-6675584047dd","metadata":{"id":"deb17d2d-2f0d-490e-864f-6675584047dd"},"outputs":[],"source":["fviz_pca_var(iris.pca,\n","             col.var = \"contrib\",\n","             gradient.cols = c(\"#E0ECF4\", \"#9EBCDA\", \"#8856A7\"),\n","             repel = TRUE\n","             )\n","\n","# plot variable loading plot, colored by relative contribution, using factoextra package"]},{"cell_type":"markdown","id":"0957111d-8d85-4a8e-81db-6b8b569626de","metadata":{"id":"0957111d-8d85-4a8e-81db-6b8b569626de"},"source":["Petal Length and Sepal Length have the strongest contribution.  Sepal Length acts along both PCs and shows a negative correlation for both.  Petal Length is almost solely associated with PC1 and is likely causing the separation of the species data point.  Petal Length, Petal Width and Sepal Length all trend in the same general direction, suggesting that these variable are correlated.  This is consistent with what we see from the scatterplots above.\n","\n","We can combine these two plots into a <b>biplot</b>."]},{"cell_type":"code","execution_count":null,"id":"5a762387-0727-435f-a1be-52bc0990a92e","metadata":{"id":"5a762387-0727-435f-a1be-52bc0990a92e"},"outputs":[],"source":["fviz_pca_biplot(iris.pca, repel = TRUE,\n","                col.ind = iris$Species\n","                )\n","\n","# plot biplot using factoextra package"]},{"cell_type":"markdown","id":"10854dac-c143-40d6-b7c9-dd2f315871ed","metadata":{"id":"10854dac-c143-40d6-b7c9-dd2f315871ed"},"source":["The PCA plot has many uses beyond identifying interesting clusters.  It is commonly used to identify batch effects in omics data which have to be accounted for in regression models (see <b>Submodule 4: Exploratory Analysis of Proteomics IRI Data</b> for an example).  PCA can also be used as a precursor to regression analysis or k-means clustering to simplify the data sets and reduce noise.  PCA is related to many other dimensionality reduction techniques (either conceptually or mathematically) such as factor analysis, discriminant analysis, multidimensional scaling, and t-SNE."]},{"cell_type":"markdown","id":"812d0ff9-07e2-489b-bdf0-c78fe2de9b6a","metadata":{"id":"812d0ff9-07e2-489b-bdf0-c78fe2de9b6a"},"source":["---"]},{"cell_type":"markdown","id":"a2d06618-9eff-4d92-9ac3-b008d39d571b","metadata":{"id":"a2d06618-9eff-4d92-9ac3-b008d39d571b"},"source":["## Clustering Algorithms"]},{"cell_type":"markdown","id":"cbbab091-0b4e-4e63-87b1-30673d639afa","metadata":{"id":"cbbab091-0b4e-4e63-87b1-30673d639afa"},"source":["PCA and other dimensionality reduction methods are often used to identify clusters of similar data points.  However, PCA looks at the entire dataset and does not take into account similarity data between data points.  Any separation is a consequence of the dimensionality reduction and will only happen if there is sufficient variance associated with the PC's.  <b>Clustering algorithms</b>, by contrast, look for relationships between the data points themselves to identify shared properties within the clusters.\n","\n","There are many ways to cluster data and many properties that we can cluster by.  Phylogenetic trees, for instance, can be built from distance matrices based on similarity between the sequences used to build the tree.  We also use dissimilarity matrices to measure ecological diversity within and between populations.  Clustering is also an important tool in machine learning.  In this submodule, we will only look at the two most common methods, both of which we've seen before with our heatmaps: <b>hierarchical clustering</b> and <b>k-means clustering</b>."]},{"cell_type":"markdown","id":"dcfabfb0-2872-4d56-a455-4ed052ca86e3","metadata":{"id":"dcfabfb0-2872-4d56-a455-4ed052ca86e3"},"source":["---"]},{"cell_type":"markdown","id":"b998114b-7efb-4486-8f3c-3644b214030c","metadata":{"id":"b998114b-7efb-4486-8f3c-3644b214030c"},"source":["### Hierarchical Clustering"]},{"cell_type":"markdown","id":"799e02b2-2f8e-4e72-a508-d05b979d8072","metadata":{"id":"799e02b2-2f8e-4e72-a508-d05b979d8072"},"source":["<b>Hierarchical Cluster Analysis (HCA)</b> is probably the most well-recognized clustering method.  This is often the basis of the classic tree structure (<b>dendrogram</b>) that we used for clustering the heatmaps above.  There are many ways to perform HCA but it's common to build a distance matrix (often based on Euclidean distance) that numerically defines how dissimilar two data points are.  We can then use that information to build a dendrogram."]},{"cell_type":"code","execution_count":null,"id":"83b68689-3bdf-4bda-8547-f359a6424ef4","metadata":{"id":"83b68689-3bdf-4bda-8547-f359a6424ef4"},"outputs":[],"source":["dist_iris <- dist(scale(iris[,1:4]), method = \"euclidean\") # calculate Euclidean distances\n","hca_iris <- hclust(dist_iris, method = \"complete\") # cluster using \"complete linkage\" method\n","dend_iris <- as.dendrogram(hca_iris) # create dendrogram\n","dend_iris_color <- color_branches(dend_iris, col = iris$Species) # color branches by Species\n","plot(dend_iris_color) # plot the dendrogram\n","rect.hclust(hca_iris, k = 3, border = c(\"#0295f7\", \"#9EBCDA\", \"#8856A7\")) # add colored boxes defining k-means clusters (k = 3)"]},{"cell_type":"markdown","id":"3be84406-01c5-4797-9b7a-092e9236b6ed","metadata":{"id":"3be84406-01c5-4797-9b7a-092e9236b6ed"},"source":["This is exactly what we saw before.  Setosa separates cleanly, virginica and versicolor not quite as cleanly.  Feel free to try different distance and clustering methods to see how it affects the results."]},{"cell_type":"markdown","id":"004d3781-bc60-46dc-af75-4ccb42b76b5c","metadata":{"id":"004d3781-bc60-46dc-af75-4ccb42b76b5c"},"source":["---"]},{"cell_type":"markdown","id":"85e758fd-af3d-4f7a-8df9-48edb390a328","metadata":{"id":"85e758fd-af3d-4f7a-8df9-48edb390a328"},"source":["### k-Means Clustering"]},{"cell_type":"markdown","id":"9db78228-29e1-480e-b07b-3a506e26ebab","metadata":{"id":"9db78228-29e1-480e-b07b-3a506e26ebab"},"source":["<b>k-Means Clustering</b> is one of the most common clustering methods.  We first dictate how many clusters (k) we expect to find in our data.  The algorithm then identifies k random \"centroids\" around which to build the clusters.  This is an iterative process that assigns data points to clusters in a way that minimizes the in-cluster variance (Total Within Sum of Squares)."]},{"cell_type":"markdown","id":"6fec639d-6f6e-4e84-9458-54eb31a7192a","metadata":{"id":"6fec639d-6f6e-4e84-9458-54eb31a7192a"},"source":["How do we choose k?  For iris, we would expect three clusters, one for each species.  If we didn't know this beforehand, however, we would have to determine the optimal number of clusters computationally.  The simplest way to do this is to use an <b>elbow plot</b>. An elbow plot is the plot of the number of clusters vs. the total within sum of squares calculated by the k-means algorithm.  The bend in the plot (the elbow) indicates the optimal number of clusters.  In this case, 3 seems like a good choice."]},{"cell_type":"code","execution_count":null,"id":"c59a1096-584a-4c88-8613-8f5997b203b6","metadata":{"tags":[],"id":"c59a1096-584a-4c88-8613-8f5997b203b6"},"outputs":[],"source":["iris_scale <- scale(iris[,1:4]) # scale the iris data\n","fviz_nbclust(iris_scale, kmeans, method = \"wss\") # create the elbow plot using k-means clustering"]},{"cell_type":"code","execution_count":null,"id":"e48c3a06-e273-4c37-b2ab-af0c69016556","metadata":{"id":"e48c3a06-e273-4c37-b2ab-af0c69016556"},"outputs":[],"source":["iris_kmeans <- kmeans(iris_scale, 3) # run the k-means algorithm on iris assuming three clusters\n","iris_kmeans"]},{"cell_type":"markdown","id":"1863dfb0-e6dd-4eb5-b757-c95673f98421","metadata":{"id":"1863dfb0-e6dd-4eb5-b757-c95673f98421"},"source":["We have three clusters of size 53, 47 and 50, respectively.  Remember that our species are evenly sized at 50, so we can already see that our clustering is not perfect.  Let's plot the data and see what it looks like."]},{"cell_type":"code","execution_count":null,"id":"4c017d6b-47d9-47bb-918c-bb73e348130c","metadata":{"id":"4c017d6b-47d9-47bb-918c-bb73e348130c"},"outputs":[],"source":["iris_distance <- get_dist(iris_scale)\n","fviz_dist(iris_distance, gradient = list(low = \"#0295f7\", mid = \"white\", high = \"#8856A7\"))"]},{"cell_type":"code","execution_count":null,"id":"662fe9af-ecea-426a-8c67-081989c8c837","metadata":{"id":"662fe9af-ecea-426a-8c67-081989c8c837"},"outputs":[],"source":["fviz_cluster(\n","    iris_kmeans, \n","    data = iris_scale,\n","    geom = \"point\",\n","    pointsize = 5,\n","    show.clust.cent = TRUE) +\n","    geom_point(aes(shape = iris$Species, size = 5), alpha = 0.5)\n","\n","# We plot with the fviz_cluster function in factoextra package\n","# We set geom to point so labels don't muck up the space\n","# We add an extra point to each cluster representing the centroid, that is, the mean of each cluster\n","# We overlay the species data as different shapes"]},{"cell_type":"markdown","id":"de95ae7c-bcaa-41e8-b75e-72bc30830790","metadata":{"id":"de95ae7c-bcaa-41e8-b75e-72bc30830790"},"source":["Once again, we see that setosa separates cleanly.  Virginica and versicolor mostly cluster separately but there is still overlap."]},{"cell_type":"markdown","id":"66c9509b-715a-4c2b-b993-733947dbafad","metadata":{"id":"66c9509b-715a-4c2b-b993-733947dbafad"},"source":["---"]},{"cell_type":"markdown","id":"04d28f46-44d1-4ad9-987f-8646f84a0663","metadata":{"id":"04d28f46-44d1-4ad9-987f-8646f84a0663"},"source":["## Conclusions"]},{"cell_type":"markdown","id":"74fca360-10f7-4467-90f6-63dbfc3e658d","metadata":{"id":"74fca360-10f7-4467-90f6-63dbfc3e658d"},"source":["Exploratory analysis is more of a philosophy than a set defined techniques, and thus we've only touched on a few of the most common techniques here and will cover them in more detail with omics data in <b>Submodule 4: Exploratory Analysis of Proteomics IRI Data</b>.  <b>Submodule 5: Identification of IRI Biomarkers from Proteomic Data</b> will also look at omics-specific plots such as MA-plots and volcano plots that are useful for getting a broad view of expression patterns."]},{"cell_type":"markdown","id":"63ffa8dc-8775-4d9a-bc0c-21fce6987371","metadata":{"id":"63ffa8dc-8775-4d9a-bc0c-21fce6987371"},"source":["---"]},{"cell_type":"markdown","source":["## Quiz\n","\n","The quiz for this chapter can be found at the GitHub repository for this module (Colab_Biomarkers/Biomarkers/quizes/Chapter4_Quizes.ipynb)"],"metadata":{"id":"vgRJcYB6BkeD"},"id":"vgRJcYB6BkeD"},{"cell_type":"markdown","id":"c56e8177-4fea-4468-a6bd-d3df657aa202","metadata":{"id":"c56e8177-4fea-4468-a6bd-d3df657aa202"},"source":["---"]},{"cell_type":"markdown","id":"7553db5b-a789-4080-b349-61adb45f874b","metadata":{"id":"7553db5b-a789-4080-b349-61adb45f874b"},"source":["## References"]},{"cell_type":"markdown","id":"2e514c00-925a-4dcd-ae0a-cb757b0de30b","metadata":{"id":"2e514c00-925a-4dcd-ae0a-cb757b0de30b"},"source":["### Core Reading\n","[Tabachnick BG, Fidell LS: Using Multivariate Statistics (7th Edition). 2021, Pearson Education Inc.][Tabachnick]<br>\n","[Everitt B, Holthorn T: An Introduction to Applied Multivariate Analysis with R. 2011, Springer Science+Business Media][Everitt]<br>\n","\n","[Tabachnick]: https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543 \"Tabachnick BG, Fidell LS: Using Multivariate Statistics (7th Edition). 2021, Pearson Education Inc.\"\n","[Everitt]: https://link.springer.com/book/10.1007/978-1-4419-9650-3 \"Everitt B, Holthorn T: An Introduction to Applied Multivariate Analysis with R. 2011, Springer Science+Business Media\""]},{"cell_type":"markdown","id":"d76ca606-4c7c-4325-a0e3-029b820c21e6","metadata":{"id":"d76ca606-4c7c-4325-a0e3-029b820c21e6"},"source":["---"]}],"metadata":{"environment":{"kernel":"ir","name":"r-cpu.4-2.m104","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/r-cpu.4-2:m104"},"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.2.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}