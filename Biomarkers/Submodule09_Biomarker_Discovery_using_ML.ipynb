{"cells":[{"cell_type":"markdown","id":"8fd15181-2723-4338-9006-79477c2d706c","metadata":{"id":"8fd15181-2723-4338-9006-79477c2d706c"},"source":["<table>\n","  <tr>\n","    <th>\n","      <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/RIINBRE-Logo.jpg\", height = \"125\", alt=\"RI-INBRE Logo\">\n","    </th>\n","    <th>\n","      <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/MIC_Logo.png\", height = \"125\", alt=\"RI-INBRE Logo\">\n","    </th>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","id":"a2b81336-599e-4794-960a-bda34596acfc","metadata":{"id":"a2b81336-599e-4794-960a-bda34596acfc"},"source":["# Analysis of Biomedical Data for Biomarker Discovery\n","## Submodule 9: Biomarker Discovery Using Machine Learning\n","### Dr. Christopher L. Hemme\n","### Director, [RI-INBRE Molecular Informatics Core](https://web.uri.edu/riinbre/mic/)\n","### The University of Rhode Island College of Pharmacy\n","Last Updated: June 5, 2023"]},{"cell_type":"markdown","id":"4d25e3ad-0488-4835-ba2e-68b5cbb27528","metadata":{"id":"4d25e3ad-0488-4835-ba2e-68b5cbb27528"},"source":["---"]},{"cell_type":"markdown","id":"750e7b90-0eef-46e3-b3bb-c6e7d2cd55f9","metadata":{"id":"750e7b90-0eef-46e3-b3bb-c6e7d2cd55f9"},"source":["## Introduction"]},{"cell_type":"markdown","id":"78e1569c-6bb8-45a4-bd37-1bdddd5b4e0d","metadata":{"id":"78e1569c-6bb8-45a4-bd37-1bdddd5b4e0d"},"source":["The public view of machine learning (influenced by Hollywood) is that of rogue artificial intelligence running wild and causing chaos.  If you watch the news, it sometimes seems like this is true in real life.  In most cases though, machine learning is simply a way to improve the efficiency of completing some set of tasks.  Several of the methods we've covered in this module - linear regression, logistic regression, principal components analysis, k-means clustering - are used in many machine learning algorithms.  The main difference between how we've used those methods previously is that in machine learning, we are taking advantage of the predictive properties of the methods to map new data to existing data to identify patterns in new data.\n","\n","It can sometimes be difficult to decipher the different buzzwords associated with the field.  Specifically, people are often confused by the terms <b>artificial intelligence (AI)</b>, <b>machine learning (ML)</b>, and <b>deep learning (DL)</b>.  AI is a broad umbrella term that simply means algorithms that can simulate some level of human intelligence, usually for performing some task such as pattern recognition.  ML is a subset of AI that specializes in algorithms that are capable of learning without being explicitly programmed.  DL is a subset of ML that specifically tries to mimic the processes of the human brain to identify and classify patterns in large data sets.  We will focus on simple ML for this submodule.\n","\n","There are several strategies used in ML, but the two most common are <b>supervised</b> and <b>unsupervised learning</b>.  In supervised learning, we use a subset of labeled data to create a training set to train our algorithm, which can then process new data based on the training set.  In unsupervised learning, the algorithm is attempting to identify new patterns or structures in unlabeled data.\n","\n","Some examples of how ML is currently or will be used in biomedical science include:\n","\n","- Diagnose a patient's disease severity based on their tissue proteomics profile\n","- Predict an individual's susceptibility to disease based on their pattern of genetic variations\n","- Solve the three-dimensional structure of an unknown protein based on its sequence\n","- Identify approved drugs which can be repurposed to treat diseases based on chemical structure and omics profiles\n","\n","In this module we will use some basic ML algorithms to analyze the IRI proteomics data we've previously analyzed."]},{"cell_type":"markdown","id":"c6525e05-b29d-4557-b1d1-9300a6fdaca0","metadata":{"id":"c6525e05-b29d-4557-b1d1-9300a6fdaca0"},"source":["<div class=\"alert alert-block alert-info\">\n","<b>&#9995; Tip:</b> Blue boxes will indicate helpful tips.</div>"]},{"cell_type":"markdown","id":"7b6a7366-74e3-40ee-bfcf-dfd693176424","metadata":{"id":"7b6a7366-74e3-40ee-bfcf-dfd693176424"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> Used for interesting asides or notes.\n","</div>"]},{"cell_type":"markdown","id":"bf2c1c8f-2d73-4b57-a4be-48cc7a2cf8d6","metadata":{"id":"bf2c1c8f-2d73-4b57-a4be-48cc7a2cf8d6"},"source":["<div class=\"alert alert-block alert-success\">\n","<b>&#9997; Reference:</b> This box indicates a reference for an attached figure or table.\n","</div>"]},{"cell_type":"markdown","id":"1e588389-ad3d-40dc-a3a9-6acd32769074","metadata":{"id":"1e588389-ad3d-40dc-a3a9-6acd32769074"},"source":["<div class=\"alert alert-block alert-danger\">\n","<b>&#128721; Caution:</b> A red box indicates potential hazards or pitfalls you may encounter.\n","</div>"]},{"cell_type":"markdown","id":"c3031c1b-d917-4460-b409-6f643b76750a","metadata":{"id":"c3031c1b-d917-4460-b409-6f643b76750a"},"source":["## Load the libraries"]},{"cell_type":"markdown","id":"f4985e37-c2d0-4733-9e2d-fcbcc169c18a","metadata":{"id":"f4985e37-c2d0-4733-9e2d-fcbcc169c18a"},"source":["This notebook creates a basic template for running a machine learning project in R. "]},{"cell_type":"code","execution_count":null,"id":"3542fc31-a20f-43d7-afd9-cccd05379a4e","metadata":{"tags":[],"id":"3542fc31-a20f-43d7-afd9-cccd05379a4e"},"outputs":[],"source":["# Install packages\n","if (!require(\"BiocManager\", quietly = TRUE))\n","    install.packages(\"BiocManager\")\n","library('BiocManager')\n","\n","bioc_packages <- c(\"limma\", \"ComplexHeatmap\", \"M3C\")\n","installed_bioc_packages <- bioc_packages %in% rownames(installed.packages())\n","if (any(installed_bioc_packages == FALSE)) {BiocManager::install(bioc_packages[!installed_bioc_packages])}\n","\n","BiocManager::install(\"preprocessCore\")\n","BiocManager::install(\"preprocessCore\", configure.args=\"--disable-threading\", force = TRUE)\n","\n","packages <- c(\"caret\", \"randomForest\", \"kernlab\", \"ggraph\", \"rattle\", \"gbm\")\n","installed_packages <- packages %in% rownames(installed.packages())\n","if (any(installed_packages == FALSE)) {install.packages(packages[!installed_packages])}\n","\n","if(!require(devtools)) install.packages(\"devtools\")\n","devtools::install_github(\"kassambara/factoextra\")"]},{"cell_type":"code","execution_count":null,"id":"3260b3ee-4b7b-4170-914a-153291a8d607","metadata":{"id":"3260b3ee-4b7b-4170-914a-153291a8d607"},"outputs":[],"source":["# Load the packages\n","require('preprocessCore')\n","require('limma')\n","require('ComplexHeatmap')\n","require('plyr')\n","require('tidyverse')\n","require('factoextra')\n","require('caret')\n","require('kernlab')\n","require('ggpubr')\n","require('rattle')\n","require('dplyr')\n","require('ggraph')\n","require('igraph')\n","require('ggplot2')\n","require('mlbench')\n","require('pROC')\n","require(\"gbm\")"]},{"cell_type":"markdown","id":"e95ecfbc-b0a1-402f-ae7b-e1edb9b26c84","metadata":{"id":"e95ecfbc-b0a1-402f-ae7b-e1edb9b26c84"},"source":["## Load the proteome data and merge with the metadata"]},{"cell_type":"markdown","id":"8e803f3a-5169-4002-92f0-53b58491819f","metadata":{"id":"8e803f3a-5169-4002-92f0-53b58491819f"},"source":["We will be loading our experimental object from <b>Submodule07 - Exploratory Proteomic Analysis</b> since it already has our proteomic data merged with our metadata and has been normalized."]},{"cell_type":"code","execution_count":null,"id":"a4f8cb05-26fb-48bd-9353-ef8f275162da","metadata":{"id":"a4f8cb05-26fb-48bd-9353-ef8f275162da"},"outputs":[],"source":["exp_obj <- readRDS(file = \"data/exp_obj.rds\")"]},{"cell_type":"markdown","id":"e8906518-b3cc-421f-a229-2f7e75de9e46","metadata":{"id":"e8906518-b3cc-421f-a229-2f7e75de9e46"},"source":["<div class=\"alert alert-block alert-danger\">\n","<b>&#128721; Caution:</b> Reminder that this version of the experimental object should contain the quantile normalized data from submodule 4.  Use <i>str(exp_obj)</i> if you need to verify that this is the case. \n","</div>"]},{"cell_type":"code","execution_count":null,"id":"09aac258-1e68-4ea9-9d51-3fb7d5fb217f","metadata":{"id":"09aac258-1e68-4ea9-9d51-3fb7d5fb217f"},"outputs":[],"source":["# Read in the normalized data from Submodule 4\n","proteome_norm <- exp_obj$data$proteomics$norm\n","metadata <- exp_obj$metadata\n","head(metadata)"]},{"cell_type":"code","execution_count":null,"id":"72be9898-4804-4c6f-a069-4711bd8921a9","metadata":{"id":"72be9898-4804-4c6f-a069-4711bd8921a9"},"outputs":[],"source":["head(proteome_norm)"]},{"cell_type":"markdown","id":"44ab211a-45e3-44e3-8c38-2e501b5eecbe","metadata":{"id":"44ab211a-45e3-44e3-8c38-2e501b5eecbe"},"source":["Next we will transpose the normalized protein data so that it is in the same orientation as the metadata file. That way we can add metadata columns directly to the data frame. We'll make a new data frame so we don't corrupt the original proteome_norm."]},{"cell_type":"code","execution_count":null,"id":"fbf65765-ab3f-4cf7-b8e4-0c37b0a4e819","metadata":{"id":"fbf65765-ab3f-4cf7-b8e4-0c37b0a4e819"},"outputs":[],"source":["t_proteome = as.data.frame(t(proteome_norm))\n","head(t_proteome)"]},{"cell_type":"markdown","id":"a3d8e32d-29bc-413b-b704-47bc4b9f6b30","metadata":{"id":"a3d8e32d-29bc-413b-b704-47bc4b9f6b30"},"source":["## Define Injury States for Machine Learning"]},{"cell_type":"markdown","id":"c764cfb2-ff74-4ec9-844e-e035f7177fd7","metadata":{"id":"c764cfb2-ff74-4ec9-844e-e035f7177fd7"},"source":["Remember that supervised machine learning involves splitting the dataset into training and testing sets. In order to do supervised machine learning, we need to have an outcome to train the algorithm to predict, in this case, predicting injury state from IRI proteomics data. We will define a sample as injured if it has a SCr concentration above a critical threshold, whereas any sample with a SCr below that will be considered healthy. As we did in <b>Submodule 6: Linear and Logistic Regression for Comparison of Quantitative Biomarkers</b>, we will use the clinically established value of SCr >= 1.  \n","\n","In the code block below, we end up with a cohort that has approximately 3 times as many healthy samples as injured samples. Class imbalances like this are common in machine learning and especially exaggerated in bioinformatics. We will have to account for this when we build our models."]},{"cell_type":"markdown","id":"2d177c90-e909-4477-ad4e-7c850371b8d9","metadata":{"id":"2d177c90-e909-4477-ad4e-7c850371b8d9"},"source":["Just as we did before in <b>Submodule06 - Linear and Logistic Regression Biomarkers</b> we will create a new column labeled **'D'** and give a value to each sample depending on whether SCr levels are greater than 1 but instead of giving 1 and 0 values we will use the terms Healthy vs. Injured."]},{"cell_type":"markdown","id":"c82d5e81-69ba-4410-91b7-7c3721b0ee92","metadata":{"id":"c82d5e81-69ba-4410-91b7-7c3721b0ee92"},"source":["<div class=\"alert alert-block alert-info\">\n","<b>&#9995; Tip:</b> In a real-world machine learning scenario, defining labels is often a painstaking process that must be done in conjunction with subject matter experts and frequently involves a prolonged back-and-forth process between the team that generated the data and the analysis team.\n","</div>"]},{"cell_type":"markdown","id":"a3437607-86a4-4fb5-b263-1d6719b0de9b","metadata":{"id":"a3437607-86a4-4fb5-b263-1d6719b0de9b"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> Some of the machine learning code may take several minutes to run.\n","</div>\n","<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> You will have to load the IRI_Biomarker.csv file into your Colab data folder.\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"0535066a-9d31-43c7-9890-d4b96feb174e","metadata":{"id":"0535066a-9d31-43c7-9890-d4b96feb174e"},"outputs":[],"source":["# Add SCR/BUN data to the metadata frame\n","biomarkers = read.csv(\"data/IRI_Biomarkers.csv\")\n","sample_data = cbind.data.frame(metadata,biomarkers)\n","\n","# Summary statistics\n","summary(sample_data)\n","\n","# Define constant normal values for SCR and BUN\n","SCR = 1\n","\n","# Define injury state based on SCR. Since BUN has so many missing values, we will only use SCR. if SCR is greater than 1 label Injured\n","injury = ifelse((sample_data$SCr > SCR),\"Injured\",\"Healthy\")\n","table(injury)\n","sample_data$D = injury"]},{"cell_type":"code","execution_count":null,"id":"a0326674-783b-4e76-9e39-7d5f355131aa","metadata":{"id":"a0326674-783b-4e76-9e39-7d5f355131aa"},"outputs":[],"source":["# Add the Injured outcome label to the proteomics data frame for training\n","\n","t_proteome$D = factor(sample_data$D)\n","head(t_proteome)"]},{"cell_type":"markdown","id":"9129ba8a-544b-41ee-a73d-d48c6c50f24b","metadata":{"tags":[],"id":"9129ba8a-544b-41ee-a73d-d48c6c50f24b"},"source":["## Preprocess and train/test split"]},{"cell_type":"markdown","id":"04f206a2-fed7-4f39-a33d-fe2381166e2d","metadata":{"id":"04f206a2-fed7-4f39-a33d-fe2381166e2d"},"source":["One of the core concepts of machine learning is the splitting of data into training and test sets. The training dataset is used to build the model and contains the data itself (proteomics counts in this example) and a set of labels (injury state). The model iterates over the training data, learning the relationships between the input data and output labels. Once the model is trained its predictive ability is evaluated using the test dataset. The test dataset should be data that the model has never seen before but that has come from the same distribution as the training data.\n","\n","We will split the data into training and test datasets comprised of 80% and 20% of the original data, respectively. The **caret** package provides functionality to do this with the **createDataPartition** function. As machine learning becomes increasingly widely adopted, the ML packages in both R and Python have developed cutting edge helper functions like this, and using them for train/test splitting is a best practice that is highly preferable to hard-coding the split with base R commands.\n","\n","One benefit of using **createDataPartition** is that it automatically handles class imbalances when splitting on a categorical variable. Remember earlier when we said we had many more healthy samples than injury samples? **createDataPartition** recognizes this and makes sure that the training set has roughly the same ratio of healthy:injured as the training set does.  Class imbalances can still affect model performance. If the class you are trying to predict is a rare event, the model can learn that to improve accuracy.  It's best to assign most predictions to the class that makes up the majority of the data it was trained on. One way to handle this is random oversampling for class imbalance which we'll explore later. "]},{"cell_type":"code","execution_count":null,"id":"ac014c85-d162-4789-bfcc-a49270a9275a","metadata":{"id":"ac014c85-d162-4789-bfcc-a49270a9275a"},"outputs":[],"source":["# Set the seed so that we end up with the same split each time we run this.\n","set.seed(3456)\n","\n","# Split the data based on the variable D.\n","trainIndex <- createDataPartition(t_proteome$D, p = .8, \n","                                  list = FALSE, \n","                                  times = 1)\n","\n","# The output is the indices of samples in the training set\n","head(trainIndex)"]},{"cell_type":"code","execution_count":null,"id":"ba4471ee-7905-4503-a67a-e7a5fa30b977","metadata":{"id":"ba4471ee-7905-4503-a67a-e7a5fa30b977"},"outputs":[],"source":["# Define training and testing as data frames\n","training = t_proteome[trainIndex,]\n","testing = t_proteome[-trainIndex,]\n","dim(training)"]},{"cell_type":"markdown","id":"4ed70600-f526-4673-ad50-486630d511f0","metadata":{"id":"4ed70600-f526-4673-ad50-486630d511f0"},"source":["## Feature selection"]},{"cell_type":"markdown","id":"1d93e05e-7c80-4bad-88b3-75d3b42953e5","metadata":{"id":"1d93e05e-7c80-4bad-88b3-75d3b42953e5"},"source":["It is not feasible to use every protein in our proteomics screening as a feature in the model. Remember that omics datasets are usually sparse, meaning many of the features  will have low information across samples and will be unhelpful for distinguishing injury from healthy samples. Additionally, from a logistical perspective, it is not reasonable to put several thousand proteins into a laboratory test to use for classification in a real-world setting. With that in mind, we need to find the top features that contain the most information and use those top proteins as variables in the model, as opposed to thousands or tens of thousands that can be measured in a typical mass spectrometry experiment. \n","\n","It is important to do this feature selection process on the training set and not the full dataset, otherwise it will bias the features based on information in the test set, which your model and features should not see until model evaluation called 'test leakage'. \n","\n","In the code block below, we will build a random forest model using all of the features and rank the features based on their impact on the model's accuracy. This will give us a basis from which to include/exclude features in the training set going forward. We will learn about random forest models and the settings below in more detail soon."]},{"cell_type":"code","execution_count":null,"id":"4a18bd77-9573-4754-ac0d-a08d533432f0","metadata":{"id":"4a18bd77-9573-4754-ac0d-a08d533432f0"},"outputs":[],"source":["fitControl_randomforest <- trainControl(# 10-fold cross-validation (CV)\n","                           method = \"repeatedcv\",\n","                           number = 10,\n","                           # Repeated ten times\n","                           repeats = 10)\n","\n","# Build random forest from full dataset\n","set.seed(123)\n","# mtry hyperparameter is tuned, ntree is not\n","mtry <- sqrt(ncol(training))\n","tunegrid <- expand.grid(.mtry=mtry)\n","rf_full <- train(D~., \n","                      data=training, \n","                      method='rf', \n","                      metric='Accuracy', \n","                      tuneGrid=tunegrid, \n","                      trControl=fitControl_randomforest)\n","print(rf_full)"]},{"cell_type":"markdown","id":"c3360fe4-6da7-4113-87d4-5ee03d03e2d2","metadata":{"id":"c3360fe4-6da7-4113-87d4-5ee03d03e2d2"},"source":["Listing the important variables with `varImp` can help us see what the top 100 important features are. We can narrow the list further to show only the top 30 important features."]},{"cell_type":"code","execution_count":null,"id":"59915c26-c21d-4b69-81ef-e7b26c03d253","metadata":{"tags":[],"id":"59915c26-c21d-4b69-81ef-e7b26c03d253"},"outputs":[],"source":["rfImp = varImp(rf_full)\n","par(pty = \"m\")\n","# Plot the top 100 proteins and top 30 proteins\n","top100 <- plot(rfImp, top = 100)\n","top30 <- plot(rfImp, top = 30)\n","\n","top100\n","top30"]},{"cell_type":"markdown","id":"fb6c1856-4742-4a7e-b127-0368e6e6e862","metadata":{"id":"fb6c1856-4742-4a7e-b127-0368e6e6e862"},"source":["We can see from this plot that not all variables are equally valuable in the model. By selecting the top features, we can hopefully capture the core information that these variables encode while avoiding overfitting the model. \n","\n","Let's grab the top 30 proteins and use those to train our models."]},{"cell_type":"code","execution_count":null,"id":"28137311-5d7a-454d-a9df-ff96a72e764e","metadata":{"id":"28137311-5d7a-454d-a9df-ff96a72e764e"},"outputs":[],"source":["nKeep = 30 # Number of features to keep\n","\n","# Get the ranked importance into a dataframe\n","ImpMeasure=data.frame(varImp(rf_full)$importance)\n","ImpMeasure$Vars<-row.names(ImpMeasure)\n","\n","# Sort the dataframe and extract the protein names (row names)\n","keepVars = rownames(ImpMeasure[order(-ImpMeasure$Overall),][1:nKeep,])\n","# Eliminate the ` character from the gene names since it messes with the matching step below\n","keepVars = gsub(\"`\",\"\",keepVars)\n","\n","# Subset the training and test datasets to only the features being used\n","keep = which(colnames(training) %in% keepVars)\n","training = training[,c(keep,ncol(training))]\n","testing = testing[,c(keep,ncol(testing))]"]},{"cell_type":"markdown","id":"6341d5ee-1df5-4eaf-b5de-2127e19e5d84","metadata":{"id":"6341d5ee-1df5-4eaf-b5de-2127e19e5d84"},"source":["## Train and Test by spot checking a handful of models\n","### Classification algorithms tested: Decision Tree, Random Forest, GBM, and SVM\n","\n","In this section, we'll test our prediction with our data using different ML algorithms. It is a best practice to assess a number of algorithms appropriate for your problem. In our case, we have identified a 1:4 class imbalance and <b>decision trees</b> are typically better suited to address this scenario. We will test with three tree-based models and one non-tree-based model and compare the performance across the four classification algorithms. All of the models we will test are supervised learning models that use classification or regression methods. For all of these models we will be using the **caret** package.\n","\n","In each model you'll see similar steps: \n","- Set a random seed number so we have similar stats each time this notebook is run\n","- Set up the training procedure \n","- Determine which hyperparameter(s) and which value sets to try in our training to find the best training model\n","- Test and make predictions with our test data\n","- View results"]},{"cell_type":"markdown","id":"254557d6-38a6-458b-bf5c-e10979071141","metadata":{"id":"254557d6-38a6-458b-bf5c-e10979071141"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> Model accuracy, while intuitive and easy to understand (thus its use in the evaluations below), is not necessarily the best metric to use for imbalanced classification problems. Depending on the problem domain you may need to optimize for other measures. See <a href=\"https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\">https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/</a> for more information.\n","</div>"]},{"cell_type":"markdown","id":"7479379a-db75-4e15-b410-d4b8572fe844","metadata":{"id":"7479379a-db75-4e15-b410-d4b8572fe844"},"source":["## Decision Tree Models"]},{"cell_type":"markdown","id":"ad9c9dd7-2241-4c85-9d35-ba4786d3aae7","metadata":{"id":"ad9c9dd7-2241-4c85-9d35-ba4786d3aae7"},"source":["As mentioned before, a <b>Decision Tree</b> is a type of supervised machine learning (as are the rest of the models in this submodule) that is used to make a prediction when asked a direct question based on categories within the dataset.  Decision trees go through the predictor variables and continuously split the data like in the example below.  When plotted it shows a single tree or hierarchical diagram that is easy to interpret.  Some terms to remember when looking at the tree are:\n","\n","- **Root node:** The base of the decision tree.\n","- **Splitting:** The process of dividing a node into multiple sub-nodes.\n","- **Decision node:** When a sub-node is further split into additional sub-nodes.\n","- **Leaf node:** When a sub-node does not further split into additional sub-nodes; represents possible outcomes.\n","- **Pruning:** The process of removing sub-nodes of a decision tree.\n","- **Branch:** A subsection of the decision tree consisting of multiple nodes.\n","\n","<div>\n","  <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/tree-graphic.png\", alt=\"tree-graphic\">\n","</div>\n"]},{"cell_type":"markdown","id":"38d2536f-ab06-46a4-aaaf-d6599e0139b8","metadata":{"id":"38d2536f-ab06-46a4-aaaf-d6599e0139b8"},"source":["<div class=\"alert alert-block alert-success\">\n","<b>&#9997; Reference:</b> <a href=\"https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/\">https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/</a>\n","</div>"]},{"cell_type":"markdown","id":"e8c7d8d5-26e4-4906-ac94-11fdc284088b","metadata":{"id":"e8c7d8d5-26e4-4906-ac94-11fdc284088b"},"source":["As we mentioned before, decision trees can predict outcomes when asked a direct question such as \"based on the following characteristics within my dataset is today more likely to be  rainy or sunny?\". In our case our question would be \"based on the characteristics within our dataset which biomarkers would lead to an injured or healthy sample?\". As the tree goes down its branches it will ask more questions such as \"does the sample also show that this biomarker is present? What is the concentration?\" until it has narrowed down its decision to whether the sample is healthy or injured."]},{"cell_type":"markdown","id":"2ce77210-b77f-4d29-a684-dccc254d0e26","metadata":{"id":"2ce77210-b77f-4d29-a684-dccc254d0e26"},"source":["<div class=\"alert alert-block alert-danger\">\n","<b>&#128721; Caution:</b> Although decision trees are great for the question we are asking they are not the best to use in large data sets or for complex questions that deal with may factors. They are also prone to overfitting and can give a low prediction accuracy rate.\n","</div>"]},{"cell_type":"markdown","id":"d4e7b718-f797-4bfc-8e90-cf6ef5ed73b7","metadata":{"id":"d4e7b718-f797-4bfc-8e90-cf6ef5ed73b7"},"source":["## Creating a Decision Tree"]},{"cell_type":"markdown","id":"9a4790e0-aeb2-4efc-b70c-b71f82984919","metadata":{"id":"9a4790e0-aeb2-4efc-b70c-b71f82984919"},"source":["Before we run our decision tree we need to set up the training procedure. In the command below we are setting a K-fold cross validation that we have set to 10 and we ask to repeat that 3 times. To understand this, think of a deck of cards as our dataset. We will be breaking our deck into 10 groups (these are our K folds).  We will train and test all sets of cards then repeat the process again another two times by shuffling our deck and again breaking the cards into 10 sets per shuffle. This will help to ensure that each iteration is different from the next which helps to avoid overfitting and helps to obtain a higher accuracy rate."]},{"cell_type":"code","execution_count":null,"id":"27b73a2d-bc22-4bb6-a60d-21f0f1fbf814","metadata":{"id":"27b73a2d-bc22-4bb6-a60d-21f0f1fbf814"},"outputs":[],"source":["# Set seed so stats don't change every time this analysis is run\n","set.seed(825)\n","\n","# Set up the training procedure or train control object, repeated Cross-Validation, that you'll refer to later in the training model\n","# Below you'll see we chose to do 10-fold cross-validation repeated 3 times \n","train.control_dt <- trainControl(\n","                           method = \"repeatedcv\",\n","                           number = 10, ## 10-fold CV\n","                           repeats = 3,## repeated three times\n","                           )"]},{"cell_type":"markdown","id":"0160b98e-eb29-4b75-9b5d-5574c7086c82","metadata":{"id":"0160b98e-eb29-4b75-9b5d-5574c7086c82"},"source":["Now we need to select our tuning parameter. Each model has a hyperparameter that can be tuned to optimize your model's architecture and get the highest rate of performance possible (aka greater accuracy). The major tuning parameter for decision trees is complexity parameter (cp).  This is the minimum improvement amount, i.e. how many branches can the tree make without increase relative error. Below we've set it to a range because adding too large of a value will result in a very small tree and too small of a value can result in overfitting and too many branches in the tree."]},{"cell_type":"code","execution_count":null,"id":"9649162b-6102-442e-ad55-a531ecda9001","metadata":{"id":"9649162b-6102-442e-ad55-a531ecda9001"},"outputs":[],"source":["# Save parameters in tune grid object that you'll refer to later in the training model\n","tune.grid_dt <- expand.grid(cp = seq(from = 0, to = .5, by = 0.001))"]},{"cell_type":"markdown","id":"20fd9fa6-3dd5-4581-88c8-e534366b38bd","metadata":{"id":"20fd9fa6-3dd5-4581-88c8-e534366b38bd"},"source":["Now we can finally train our model. We will be using the **rpart** function within the **caret** package to train our decision tree based on the **'D'** column within our training dataset. You can see that we have added our parameters that we created before: training procedures and setting our cp grid to gain the best accuracy rate."]},{"cell_type":"code","execution_count":null,"id":"712b73dc-24fc-4696-9f4f-54fe0f8974ce","metadata":{"id":"712b73dc-24fc-4696-9f4f-54fe0f8974ce"},"outputs":[],"source":["model_train_dt <- train(D~., data = data.frame(training), \n","                 method = \"rpart\", \n","                 tuneGrid = tune.grid_dt,\n","                 trControl = train.control_dt,\n","                 metric = \"Accuracy\"\n","               )"]},{"cell_type":"code","execution_count":null,"id":"1f9e658b-87dd-48de-9acf-a81624615a15","metadata":{"tags":[],"id":"1f9e658b-87dd-48de-9acf-a81624615a15"},"outputs":[],"source":["plot(model_train_dt)"]},{"cell_type":"markdown","id":"f6da2dbf-5aba-47bb-8117-dd47acb01584","metadata":{"tags":[],"id":"f6da2dbf-5aba-47bb-8117-dd47acb01584"},"source":["Now we tune our model with the best tuning parameter."]},{"cell_type":"code","execution_count":null,"id":"46047eb9-fcd8-4607-8346-5ede0f9a72f9","metadata":{"scrolled":true,"tags":[],"id":"46047eb9-fcd8-4607-8346-5ede0f9a72f9"},"outputs":[],"source":["# Show best tuning parameter from all the ones we tested\n","# Print the best tuning parameter cp that maximizes model accuracy\n","model_train_dt$bestTune\n","\n","# Show the model \n","model_train_dt\n"]},{"cell_type":"markdown","id":"a2b38c59-4b1b-4b39-8d9b-fa9d7aead79e","metadata":{"id":"a2b38c59-4b1b-4b39-8d9b-fa9d7aead79e"},"source":["From our plot and the table we can see that the best cp value with the highest accuracy of 75.3% is 0.222."]},{"cell_type":"markdown","id":"82681835-5662-41c2-ad3c-ffcb6ddae755","metadata":{"id":"82681835-5662-41c2-ad3c-ffcb6ddae755"},"source":["We can plot our tree based on our training dataset by using the **rattle** package to add some color. Looks like it found a simple pattern, samples that have biomarkers Thbs at certain concentrations present may indicate injury. \n","\n","To explain the number within the decision nodes if the protein Thbs is present in the sample and the concentration is less than -0.52 mg/dL then there is a .73% chance that the sample is healthy. But if it has a greater concentration then there is a 27% chance the sample will be injured. The overall prediction for this biomarker has a 100% accuracy."]},{"cell_type":"code","execution_count":null,"id":"021dccc7-78bf-4752-ac7b-e90e2dfc0c0e","metadata":{"id":"021dccc7-78bf-4752-ac7b-e90e2dfc0c0e"},"outputs":[],"source":["fancyRpartPlot(model_train_dt$finalModel)"]},{"cell_type":"markdown","id":"0a17e355-c9d0-429a-b07e-c19367be372b","metadata":{"id":"0a17e355-c9d0-429a-b07e-c19367be372b"},"source":["Now we can finally test our model using the **predict** function. You will notice that we run **predict** twice.  The second time is to gather all prediction possibilities that we can later plot in our ROC/AUC curves. We will also create a <b>Confusion Matrix</b> that evaluates our models by showing us how many predictions were true positives, false positives, true negatives, and false negatives. We will go into more detail about confusion matrices towards the end of this submodule."]},{"cell_type":"code","execution_count":null,"id":"86fbcc5e-060b-48ea-b01e-b9be6484f982","metadata":{"id":"86fbcc5e-060b-48ea-b01e-b9be6484f982"},"outputs":[],"source":["# Testing - Returns Predictions\n","model_test_dt <- predict(object = model_train_dt,\n","          newdata = data.frame(testing))\n","# Testing - Returns Probabilities used in ROC/AUC curves\n","model_test_probs_dt <- predict(object = model_train_dt,\n","          newdata = data.frame(testing), type = \"prob\")\n","\n","\n","# Print confusion matrix  \n","confusionMatrix(data = model_test_dt,\n","                  reference = testing$D,\n","                  positive = \"Injured\")"]},{"cell_type":"markdown","id":"9c75be24-9d98-4024-adf4-12544a16de8e","metadata":{"id":"9c75be24-9d98-4024-adf4-12544a16de8e"},"source":["Our results show that our model predicted 4 Injured samples as true positive, and the overall accuracy of our model was 79%."]},{"cell_type":"markdown","id":"a6b0fb66-f257-47fc-b240-228321b255b1","metadata":{"id":"a6b0fb66-f257-47fc-b240-228321b255b1"},"source":["---"]},{"cell_type":"markdown","id":"5b65f732-9988-4f98-9df2-caed37821d7f","metadata":{"id":"5b65f732-9988-4f98-9df2-caed37821d7f"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>&#127891; Note:</b> The remaining models follow the same steps as the decision tree. Though they do use different methods and tuning parameters.\n","</div>\n"]},{"cell_type":"markdown","id":"482cceb4-f875-4d8c-9d74-ffbeaeab71f2","metadata":{"id":"482cceb4-f875-4d8c-9d74-ffbeaeab71f2"},"source":["## Random Forest Model"]},{"cell_type":"markdown","id":"f9a48cf8-247b-422b-a38c-e2c22d600a85","metadata":{"id":"f9a48cf8-247b-422b-a38c-e2c22d600a85"},"source":["<b>Random Forests</b> consists of hundreds or thousands of decision trees, each containing different outcomes or predictions. It then takes the average of these trees based on their accuracy to present the best prediction with the least amount of errors. Random forests have higher accuracy and can answer complex questions like \"based on the following characteristics within my dataset will today more likely be a rainy, sunny, cloudy, or windy day?\"."]},{"cell_type":"markdown","id":"bd768571-4847-494b-8667-e88d77ae2e55","metadata":{"id":"bd768571-4847-494b-8667-e88d77ae2e55"},"source":["Random forests use **mtry** to tune their models. Since the trees are made independently from each other we need to create some randomness and thats exactly what mtry does!"]},{"cell_type":"code","execution_count":null,"id":"8b78c379-4dcd-4bea-9757-7682cc49f4c3","metadata":{"id":"8b78c379-4dcd-4bea-9757-7682cc49f4c3"},"outputs":[],"source":["# Set seed so stats don't change every time this analysis is run\n","set.seed(123)\n","\n","# SELECT TUNING PARAMETERS\n","# The major tuning parameter for the random forest model is mtry. This determines how many predictors are used at each split.\n","# A common value for default is the square root of the number of predictors in software packages. \n","\n","# Define mtry values you'd like to assess\n","sqrt(ncol(training))\n","\n","mtry <- c(4.5, sqrt(ncol(training)), 6.5, 7.5, 8.5, 9.5)\n","          \n","# Save parameters in tune grid object that you'll refer to later in the training model\n","tunegrid <- expand.grid(.mtry=mtry)\n","\n"]},{"cell_type":"markdown","id":"fd48908e-c658-4b50-bab0-380166df7f72","metadata":{"id":"fd48908e-c658-4b50-bab0-380166df7f72"},"source":["<div class=\"alert alert-block alert-info\">\n","<b>&#9995; Tip:</b>\n","\n","If there are many relevant predictor variables, mtry should be set small so that the strongest influential variables are chosen in the splits alongside less influential variables, which can provide small but relevant performance gains. These less influential variables might, for example, be useful for the prediction of a small group of observations that stronger variables fail to predict correctly. When mtry is large less influential variables might not have the chance to contribute to prediction because stronger variables are preferably selected for splitting and thus “mask” the smaller effects. Genomics datasets often have a few relevant variables compared to many so it may be a good idea to try mtry values that are higher. Source: https://arxiv.org/pdf/1804.03515.pdf#:~:text=2.1.1%20Number%20of%20randomly,Lower%20values%20of%20mtry%20lead \n","</div>"]},{"cell_type":"markdown","id":"8e7e8bd3-de03-4740-893b-df6e87ce6cea","metadata":{"id":"8e7e8bd3-de03-4740-893b-df6e87ce6cea"},"source":["#### Random Forest Training"]},{"cell_type":"markdown","id":"51cef7b5-8e2f-4fac-8e7a-fae7f09ca729","metadata":{"id":"51cef7b5-8e2f-4fac-8e7a-fae7f09ca729"},"source":["For random forest we are using the **rf** method from the **caret** package."]},{"cell_type":"code","execution_count":null,"id":"df226ab5-f8f5-4498-8d83-cf8d527cf6a9","metadata":{"id":"df226ab5-f8f5-4498-8d83-cf8d527cf6a9"},"outputs":[],"source":["# Set up the training procedure or train control object, repeated Cross-Validation, that you'll refer to later in the training model\n","# Below you'll see we chose to do 10-fold cross-validation repeated 3 times \n","train_control_rf <- trainControl(\n","                           method = \"repeatedcv\",\n","                           number = 10,\n","                           repeats = 3)\n","\n","# Fit the model \n","model_train_rf <- train(D~., \n","                      data=training, \n","                      method='rf', \n","                      metric='Accuracy', \n","                      tuneGrid=tunegrid, \n","                      trControl=train_control_rf)\n","\n","# View the model\n","plot(model_train_rf)\n","\n","# Show best tuning parameter from all the ones we tested\n","# Print the best tuning parameter C that maximizes model accuracy\n","model_train_rf$bestTune\n","\n","# Show the model \n","model_train_rf"]},{"cell_type":"markdown","id":"c9c008f9-9e8c-4b74-8be5-cb30a87c12d0","metadata":{"id":"c9c008f9-9e8c-4b74-8be5-cb30a87c12d0"},"source":["We can see from our our model and the plot that when mtry = 8.5 we get the best accuracy (83.9%)."]},{"cell_type":"markdown","id":"066c4a3b-a6b5-473b-b444-be46b5058c3f","metadata":{"id":"066c4a3b-a6b5-473b-b444-be46b5058c3f"},"source":["#### Optional: Taking a Closer Look at a Random Forest"]},{"cell_type":"markdown","id":"d189491f-2e08-4913-a53f-34581a17d490","metadata":{"id":"d189491f-2e08-4913-a53f-34581a17d490"},"source":["As we know random forests are made up of hundreds of decision trees.  To help understand what one of them would look like, let's plot one! \n","\n","First we need to know which tree to choose. If we plot the final model the plot shows us the average error rate of our classes (Injured vs. Healthy). We can see that 500 decision trees have been created."]},{"cell_type":"code","execution_count":null,"id":"9be93c9c-8caf-4dcd-a305-474e79c84b62","metadata":{"tags":[],"id":"9be93c9c-8caf-4dcd-a305-474e79c84b62"},"outputs":[],"source":["# View output matrix to ensure the legend identifies the correct class\n","tail(plot(model_train_rf$finalModel))\n","# Plot OOB error plot\n","plot(model_train_rf$finalModel)\n","    legend(\"topright\",fill=c(\"green\",\"black\", \"red\"), c( \"Injured\", \"OOB error\", \"Healthy\"))"]},{"cell_type":"markdown","id":"d9c2100d-f800-47b1-a8c8-33b7638575ec","metadata":{"id":"d9c2100d-f800-47b1-a8c8-33b7638575ec"},"source":["We can map any of these trees from the plot above but it's best to map one that has a low error value. Tree 450 looks like it has low error rates for both classes.\n","\n","Before we plot anything we first need to set the final model of our random forest then we use the **forest** function to visualize the tree. "]},{"cell_type":"code","execution_count":null,"id":"1b8a5d35-18ce-460b-9e42-29395cdeba41","metadata":{"id":"1b8a5d35-18ce-460b-9e42-29395cdeba41"},"outputs":[],"source":["final_model <- model_train_rf$finalModel$forest"]},{"cell_type":"markdown","id":"9c45131c-66db-4c32-b3c7-eb9c52702b73","metadata":{"id":"9c45131c-66db-4c32-b3c7-eb9c52702b73"},"source":["Next we will run the command below to extract and structure our tree."]},{"cell_type":"code","execution_count":null,"id":"d8a1c43d-0a7f-4586-808c-2c66b495a1f9","metadata":{"tags":[],"id":"d8a1c43d-0a7f-4586-808c-2c66b495a1f9"},"outputs":[],"source":["tree_func <- function(final_model, \n","                      tree_num) {\n","  \n","  # Get tree by index\n","  tree <- randomForest::getTree(final_model, \n","                                k = tree_num, \n","                                labelVar = TRUE) %>%\n","    tibble::rownames_to_column() %>%\n","    # Make leaf split points to NA, so the 0s won't get plotted\n","    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))\n","  \n","  # Prepare data frame for graph\n","  graph_frame <- data.frame(from = rep(tree$rowname, 2),\n","                            to = c(tree$`left daughter`, tree$`right daughter`))\n","  \n","  # Convert to graph and delete the last node that we don't want to plot\n","  graph <- graph_from_data_frame(graph_frame) %>%\n","    delete_vertices(\"0\")\n","  \n","  # Set node labels\n","  V(graph)$node_label <- gsub(\"_\", \" \", as.character(tree$`split var`))\n","  V(graph)$leaf_label <- as.character(tree$prediction)\n","  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))\n","  \n","  # Plot\n","  plot <- ggraph(graph, 'dendrogram') + \n","    theme_bw() +\n","    geom_edge_link() +\n","    geom_node_point() +\n","    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +\n","    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = \"white\") +\n","    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, \n","                    repel = TRUE, colour = \"white\", fontface = \"bold\", show.legend = FALSE) +\n","    theme(panel.grid.minor = element_blank(),\n","          panel.grid.major = element_blank(),\n","          panel.background = element_blank(),\n","          plot.background = element_rect(fill = \"white\"),\n","          panel.border = element_blank(),\n","          axis.line = element_blank(),\n","          axis.text.x = element_blank(),\n","          axis.text.y = element_blank(),\n","          axis.ticks = element_blank(),\n","          axis.title.x = element_blank(),\n","          axis.title.y = element_blank(),\n","          plot.title = element_text(size = 18))\n","  \n","  print(plot)\n","}"]},{"cell_type":"markdown","id":"b4df3efb-3ac8-49d3-9e44-35356285af4b","metadata":{"id":"b4df3efb-3ac8-49d3-9e44-35356285af4b"},"source":["Here we are setting the tree number to the number we decided before and voila! "]},{"cell_type":"code","execution_count":null,"id":"4eea5d6b-9361-4bdb-a0bf-3fb1a0731d8c","metadata":{"tags":[],"id":"4eea5d6b-9361-4bdb-a0bf-3fb1a0731d8c"},"outputs":[],"source":["tree_num=450 # Tree number\n","suppressWarnings(tree_func(final_model =  model_train_rf$finalModel, tree_num)) #displaying the plot"]},{"cell_type":"markdown","id":"0d91b242-78f0-4594-a0f8-040863670f99","metadata":{"id":"0d91b242-78f0-4594-a0f8-040863670f99"},"source":["#### Random Forest Testing"]},{"cell_type":"code","execution_count":null,"id":"c129259f-7f2d-4622-9265-26889f837088","metadata":{"id":"c129259f-7f2d-4622-9265-26889f837088"},"outputs":[],"source":["# Testing - Returns Predictions\n","model_test_rf <- predict(object = model_train_rf,\n","          newdata = testing)\n","# Testing - Returns Probabilities used in ROC/AUC curves\n","model_test_probs_rf <- predict(object = model_train_rf,\n","          newdata = testing, type = \"prob\")\n","\n","\n","# Print confusion matrix  \n","confusionMatrix(data = model_test_rf,\n","                  reference = testing$D,\n","                  positive = \"Injured\")"]},{"cell_type":"markdown","id":"2462d85f-0a49-4919-9d91-5c5178b19972","metadata":{"id":"2462d85f-0a49-4919-9d91-5c5178b19972"},"source":["Looking at the confusion matrix our random forest model was able to predict one true positive for injured samples and has a accuracy of 73.7%."]},{"cell_type":"markdown","id":"cccf494e-6a71-4ef8-8c06-eb2d30e206c5","metadata":{"id":"cccf494e-6a71-4ef8-8c06-eb2d30e206c5"},"source":["---"]},{"cell_type":"markdown","id":"c4289883-18ef-45ac-bf7c-2c7a679842b9","metadata":{"tags":[],"id":"c4289883-18ef-45ac-bf7c-2c7a679842b9"},"source":["## GBM Model\n","<b>Gradient Boosting Machines (GBM)</b> are similar to random forests in that they both consist of hundreds of decision trees. The difference between the two is that GBM models create one tree at a time so that each tree can build on each other by looking at the previous one and learning where the errors lie, leading to the next tree having better accuracy with its prediction. As we mentioned before, random forests create trees individually by randomly selecting which features the tree should contain then asses all trees together to see how to get the higher accuracy rate. "]},{"cell_type":"markdown","id":"a0052e89-3890-43b5-8732-9c3d3f54cf76","metadata":{"id":"a0052e89-3890-43b5-8732-9c3d3f54cf76"},"source":["We are using **carets** default for tuning our GBM model."]},{"cell_type":"markdown","id":"208a8a13-6db8-4327-8182-9d0a378921e8","metadata":{"id":"208a8a13-6db8-4327-8182-9d0a378921e8"},"source":["#### GBM Training "]},{"cell_type":"code","execution_count":null,"id":"7bdc2322-7d99-4985-912b-79aea3dc17b0","metadata":{"id":"7bdc2322-7d99-4985-912b-79aea3dc17b0"},"outputs":[],"source":["# Set seed so stats don't change every time this analysis is run\n","set.seed(156)\n","\n","# Set up the training procedure or train control object, repeated Cross-Validation, that you'll refer to later in the training model\n","# Below you'll see we chose to do 10-fold cross-validation repeated 3 times \n","\n","train_control_gbm <- trainControl(## 10-fold CV\n","                           method = \"repeatedcv\",\n","                           number = 10,\n","                           ## Repeated ten times\n","                           repeats = 10)\n","\n","# Fit the model \n","model_train_gbm <- train(D ~ ., data = training, \n","                 method = \"gbm\", \n","                 trControl = train_control_gbm,\n","                 ## This last option is actually one\n","                 ## for gbm() that passes through\n","                 verbose = FALSE)\n","\n","# View the model\n","plot(model_train_gbm)\n","\n","# Show best tuning parameter from all the ones we tested\n","# Print the best tuning parameter interaction.depth that maximizes model accuracy\n","model_train_gbm$bestTune\n","\n","# Show the model \n","model_train_gbm"]},{"cell_type":"markdown","id":"66b5f1f6-a83f-49a0-afe3-c9a4ac422e0a","metadata":{"id":"66b5f1f6-a83f-49a0-afe3-c9a4ac422e0a"},"source":["We can see that tuning GBM has four factors instead of one compared to the other models. \n","\n","- **n.trees** - Number of iterations\n","- **interaction.depth** - How complex the tree should be\n","- **shrinkage** - How quickly the model can adapt to get a lower error rate each time a tree is created\n","- **n.minobsinnode** - The minimum number of observations in each node this can limit the number of times the nodes can split."]},{"cell_type":"markdown","id":"b627e6cf-24e1-4b60-819c-8a2261a99102","metadata":{"id":"b627e6cf-24e1-4b60-819c-8a2261a99102"},"source":["#### GBM Testing "]},{"cell_type":"code","execution_count":null,"id":"5d3c0c66-70ae-49b1-90fa-aa4ac03ef078","metadata":{"id":"5d3c0c66-70ae-49b1-90fa-aa4ac03ef078"},"outputs":[],"source":["# Testing\n","model_test_gbm <- predict(object = model_train_gbm,\n","          newdata = testing)\n","\n","model_test_probs_gbm <- predict(object = model_train_gbm,\n","          newdata = testing, type = \"prob\")\n","\n","# Print confusion matrix  \n","confusionMatrix(data = model_test_gbm,\n","                  reference = testing$D,\n","                  positive = \"Injured\")"]},{"cell_type":"markdown","id":"746f6bf2-a506-456c-8044-361d05220ac7","metadata":{"id":"746f6bf2-a506-456c-8044-361d05220ac7"},"source":["Our GBM model was able to predict two true positive injured sample with a 84% accuracy."]},{"cell_type":"markdown","id":"3783a8ee-4ee2-4c20-b4f1-f9e91e007726","metadata":{"id":"3783a8ee-4ee2-4c20-b4f1-f9e91e007726"},"source":["---"]},{"cell_type":"markdown","id":"8d7d40b0-db92-4c18-84ca-3a0a46583ba8","metadata":{"tags":[],"id":"8d7d40b0-db92-4c18-84ca-3a0a46583ba8"},"source":["## SVM Model"]},{"cell_type":"markdown","id":"40263772-4cfa-43b9-b6fe-cdee5acd8c76","metadata":{"id":"40263772-4cfa-43b9-b6fe-cdee5acd8c76"},"source":["<b>Support Vector Machine (SVM)</b> models are similar to decision trees in the sense that they are regularly used for classification problems where the data is sparse and work well when looking at two classes. Classification is performed by finding the hyper-plane that best differentiates between the two classes. For SVM we are using the **svmLinear** method from the **caret** package."]},{"cell_type":"markdown","id":"e9ea5d20-7a7a-4ed6-a14f-5a99ffdac131","metadata":{"id":"e9ea5d20-7a7a-4ed6-a14f-5a99ffdac131"},"source":["The hyperparameter to tune for SVM models is the regularization parameter (C).  This parameter adds a penalty every time an extreme parameter creates overfitting, which is usually related to large C values.  This is why it's better to set a range of small values to tune your model. "]},{"cell_type":"markdown","id":"53d70374-fe10-455c-917b-3d3a4c9b5035","metadata":{"id":"53d70374-fe10-455c-917b-3d3a4c9b5035"},"source":["#### SVM Training "]},{"cell_type":"code","execution_count":null,"id":"d1539cc4-edfe-44b2-b053-d2dd4044f7cd","metadata":{"id":"d1539cc4-edfe-44b2-b053-d2dd4044f7cd"},"outputs":[],"source":["# Set seed so stats don't change every time this analysis is run\n","set.seed(2019)\n","\n","# SELECT TUNING PARAMETERS\n","# The major tuning parameter for the SVM Linear model is Cost. It essentially imposes a penalty to the model for making an error. \n","# The higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.\n","\n","# Make a list of cost penalty values you'de like to assess\n","costs_svm <- c(0.25, 0.4, 0.5, 0.6, 1, 1.2, 1.8,2.0)\n","\n","# Save parameters in tune grid object that you'll refer to later in the training model\n","# To skip the step above and generate a list you could use: tune_grid_svm <- expand.grid(C = seq(0, 4, length = 20))\n","tune_grid_svm <- expand.grid(C = costs_svm)\n","\n","# Set up train control object, k-fold Cross-Validation, that you'll refer to later in the training model\n","train_control_svm <- trainControl(method=\"repeatedcv\", \n","                                  number=10, \n","                                  repeats=3,\n","                                  savePredictions = TRUE,\n","                                  classProbs = TRUE)\n","\n","# Fit the model \n","model_train_svm <- train(D~., \n","              data = training, \n","              method = \"svmLinear\", \n","              tuneGrid = tune_grid_svm,\n","              trControl = train_control_svm,  \n","              preProcess = c(\"center\",\"scale\"),\n","              metric='Accuracy')\n","\n","# View the model\n","plot(model_train_svm)\n","\n","# Show best tuning parameter from all the ones we tested\n","# Print the best tuning parameter C that maximizes model accuracy\n","model_train_svm$bestTune\n","\n","# Show the model \n","model_train_svm"]},{"cell_type":"markdown","id":"ee9064ca-559d-4ce7-aa76-3b086343315b","metadata":{"id":"ee9064ca-559d-4ce7-aa76-3b086343315b"},"source":["We can see from the graph and the table that the best value for C is 1.8 and it also has the best accuracy of 79.7%."]},{"cell_type":"markdown","id":"594f2f5f-3fa0-4f57-b6ae-95fe0e8e24dd","metadata":{"id":"594f2f5f-3fa0-4f57-b6ae-95fe0e8e24dd"},"source":["#### SVM Testing "]},{"cell_type":"code","execution_count":null,"id":"df22090b-b828-47c1-baca-98c8e04e8772","metadata":{"id":"df22090b-b828-47c1-baca-98c8e04e8772"},"outputs":[],"source":["# Testing\n","model_test_svm <- predict(object = model_train_svm,\n","          newdata = testing)\n","\n","model_test_probs_svm <- predict(object = model_train_svm,\n","          newdata = testing, type = \"prob\")\n","\n","\n","# Print confusion matrix  \n","confusionMatrix(data = model_test_svm,\n","                  reference = testing$D,\n","                  positive = \"Injured\")"]},{"cell_type":"markdown","id":"f1220b9d-6e60-4657-a793-9771a30cdc60","metadata":{"id":"f1220b9d-6e60-4657-a793-9771a30cdc60"},"source":["Testing our SVM model shows that it was able to predict one true positive injured sample and has a 73.7% accuracy."]},{"cell_type":"markdown","id":"a60bd3e2-28d0-4bc4-a207-74ab5b50d504","metadata":{"id":"a60bd3e2-28d0-4bc4-a207-74ab5b50d504"},"source":["---"]},{"cell_type":"markdown","id":"fb01e709-acef-4474-92d5-e92af0c39914","metadata":{"id":"fb01e709-acef-4474-92d5-e92af0c39914"},"source":["# Comparing Our Models With Metrics\n","\n","In the previous section, we printed confusion matrices.  Here we give a deeper explanation of confusion matrices and ROC/AUC plots. "]},{"cell_type":"markdown","id":"6061256b-3041-4cf8-afa5-7bc43ec19dc3","metadata":{"id":"6061256b-3041-4cf8-afa5-7bc43ec19dc3"},"source":["<div>\n","  <img src=\"https://raw.githubusercontent.com/riinbre-bioinfo/Colab_Biomarkers/main/Biomarkers/images/ConfusionMatrix.png\", alt=\"Confusion Matrix\">\n","</div>\n"]},{"cell_type":"markdown","id":"73d9bd31-b68e-4c5d-9b40-5d99ddae5f16","metadata":{"id":"73d9bd31-b68e-4c5d-9b40-5d99ddae5f16"},"source":["Above we define a confusion matrix and common metrics calculated from its components. Most are familiar with the terms true positives vs. false positives and true negatives vs. false negatives. In our case a true positive is when we are labeling or predicting an injured sample as injured whereas a false positive is when we are labeling a sample that is really healthy as injured. ROC curves plot the false positive rate against the true positive rate. True positive rate is also known as sensitivity while false positive rate = (1-specificity). The line on a ROC curve represents the ROC while the area under the curve is the AUC and is an indicator of performance. A higher AUC indicates better performance of the model. Here we are plotting all four machine learning models to compare their performances."]},{"cell_type":"code","execution_count":null,"id":"f5ad8787-f77f-420b-bcc6-e7ebd382bdef","metadata":{"id":"f5ad8787-f77f-420b-bcc6-e7ebd382bdef"},"outputs":[],"source":["# Create ROC Curve for all models\n","\n","pROC_DT <- roc(testing$D, predictor = model_test_probs_dt[, \"Injured\"], plot=TRUE, legacy.axes=TRUE, main=\"Decision Tree\", xlab=\"False Positive Rate\", ylab=\"True Postive Rate\", col=\"#c45c12\", lwd=4, print.auc=TRUE)\n","pROC_RF <- roc(testing$D, predictor = model_test_probs_rf[, \"Injured\"], plot=TRUE, legacy.axes=TRUE, main=\"Random Forest\", xlab=\"False Positive Rate\", ylab=\"True Postive Rate\", col=\"#4daf4a\", lwd=4, print.auc=TRUE)\n","pROC_GBM <- roc(testing$D, predictor = model_test_probs_gbm[, \"Injured\"], plot=TRUE, legacy.axes=TRUE,  main=\"GBM\", xlab=\"False Positive Rate\", ylab=\"True Postive Rate\", col=\"#764aaf\", lwd=4, print.auc=TRUE)\n","pROCSVM <- roc(testing$D, predictor = model_test_probs_svm[, \"Injured\"], plot=TRUE, legacy.axes=TRUE, main=\"SVM\", xlab=\"False Positive Rate\", ylab=\"True Postive Rate\", col=\"#377eb8\", lwd=4, print.auc=TRUE)"]},{"cell_type":"markdown","id":"c75d2f86-19dd-4bb0-892a-7397d6ffcb8b","metadata":{"id":"c75d2f86-19dd-4bb0-892a-7397d6ffcb8b"},"source":["Above, we see that the model with the highest AUC was our Random Forest with a value of 0.914."]},{"cell_type":"markdown","id":"63233261-21cd-4b2c-b9c5-6e339eb4b796","metadata":{"id":"63233261-21cd-4b2c-b9c5-6e339eb4b796"},"source":["# Conclusion"]},{"cell_type":"markdown","id":"dec71dac-c1d0-4c50-b863-2ee8c8375371","metadata":{"id":"dec71dac-c1d0-4c50-b863-2ee8c8375371"},"source":["Overall we can see that our Random Forest gave the best performance having been able to predict two true positive injured samples and having a AUC of 0.914. This goes to show that even though the decision tree was able to correctly predict four injured samples our graph shows it had the worst AUC out of our models. Our SVM and GBM models tied with a AUC of 0.9 though this is a high score looking at the confusion matrix after testing the models showed that they only predicted one true positive injured sample. This means that the models guessed mostly healthy because they noticed that this resulted in a lower error rate not because they truly understood how to predict injured samples, that is another reason why they were not good models to use for our dataset."]},{"cell_type":"markdown","id":"2a2227e8-071e-4d3c-ace1-6f8104dffa68","metadata":{"id":"2a2227e8-071e-4d3c-ace1-6f8104dffa68"},"source":["<p><span style=\"font-size: 30px\"><b>Quizzes</b></span> <span style=\"float : inline;\">(run the command below to display the quizzes)</span> </p>"]},{"cell_type":"code","execution_count":null,"id":"c79c72af-76d9-4026-a1a1-199acab82714","metadata":{"id":"c79c72af-76d9-4026-a1a1-199acab82714"},"outputs":[],"source":["IRdisplay::display_html('<iframe src=\"quizes/Chapter9_Quizes.html\" width=100% height=450></iframe>')"]},{"cell_type":"markdown","id":"b56273c8-7d6c-4e62-9b38-a0260e86c39f","metadata":{"id":"b56273c8-7d6c-4e62-9b38-a0260e86c39f"},"source":["---"]},{"cell_type":"markdown","id":"6ec4e40e-9ded-4a56-8ce8-340c55ecaa65","metadata":{"id":"6ec4e40e-9ded-4a56-8ce8-340c55ecaa65"},"source":["# The End!"]},{"cell_type":"markdown","id":"55f8de17-a8e2-48eb-a0b3-bbee68ecb6f9","metadata":{"id":"55f8de17-a8e2-48eb-a0b3-bbee68ecb6f9"},"source":["Thank you for completing this module!  We hope you found it helpful and encourage you to look at other modules available in the NIH sandbox.  If you have any comments on this module, please send an email to hemmecl@uri.edu (please put NOSI Biomarker Project in the subject line).\n","\n","This project was funded as an administrative supplement to the Rhode Island INBRE grant P20GM10343 and would not have been possible without the help of numerous individuals.  Specifically:\n","\n","- Dr. Nisanne Ghonem and her laboratory the the Department of Biomedical and Pharmaceutical Sciences, College of Pharmacy, University of Rhode Island for allowing us to use their data for this module.\n","- Dr. Bongsup Cho, Program Director of RI-INBRE\n","- Lakshmi Matukumalli, Program Director, Networks and Development Programs, NIGMS/NIH for providing this opportunity\n","- The teams at Google and Deloitte (Laura Beaudry, J.T. Turner, Daniel Pan, Allen Kim, Zelaikha Yosufzai, Marcia Price) who provided much needed technical support in developing this module"]},{"cell_type":"markdown","id":"1ab70ade-9e5d-4e1b-bd18-7e38d2e4d658","metadata":{"id":"1ab70ade-9e5d-4e1b-bd18-7e38d2e4d658"},"source":["---"]}],"metadata":{"environment":{"kernel":"ir","name":"r-cpu.4-2.m104","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/r-cpu.4-2:m104"},"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.2.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}